{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\RYK\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./models/model_1000_10_cbow', './models/model_1000_10_skipgram', './models/model_1000_4_cbow', './models/model_1000_4_skipgram', './models/model_300_10_cbow', './models/model_300_10_skipgram', './models/model_300_4_cbow', './models/model_300_4_skipgram', './models/model_600_10_cbow', './models/model_600_10_skipgram', './models/model_600_4_cbow', './models/model_600_4_skipgram']\n"
     ]
    }
   ],
   "source": [
    "def find_the_way():\n",
    "    path = './models/'\n",
    "    models_files = []\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if '.npy' not in file:\n",
    "                models_files.append(os.path.join(r, file))  \n",
    "    return models_files\n",
    "models_files=find_the_way()\n",
    "print(models_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yerdomuzu': 'yerdo',\n",
       " 'antilop': 'antilop',\n",
       " 'bizon': 'bizo',\n",
       " 'deve': 'dev',\n",
       " 'bukalemun': 'bukale',\n",
       " 'çita': 'çita',\n",
       " 'şempanze': 'şempanze',\n",
       " 'kobra': 'kobra',\n",
       " 'afrikageyiği': 'afrikageyik',\n",
       " 'fil': 'fil',\n",
       " 'ceylan': 'ceyla',\n",
       " 'zürafa': 'züraf',\n",
       " 'öküzbaşlıantilop': 'öküzbaşlıantilop',\n",
       " 'goril': 'goril',\n",
       " 'suaygırı': 'suaygır',\n",
       " 'sırtlan': 'sırtla',\n",
       " 'impala': 'impal',\n",
       " 'çakal': 'çakal',\n",
       " 'lemur': 'lemur',\n",
       " 'leopar': 'leopar',\n",
       " 'aslan': 'asla',\n",
       " 'denizineği': 'denizinek',\n",
       " 'firavunfaresi': 'firavunfares',\n",
       " 'maymun': 'maym',\n",
       " 'devekuşu': 'devekuş',\n",
       " 'panter': 'panter',\n",
       " 'gergedan': 'gergeda',\n",
       " 'kaplan': 'kapla',\n",
       " 'afrikaantilobu': 'afrikaantilop',\n",
       " 'yabandomuzu': 'yabando',\n",
       " 'zebra': 'zebra',\n",
       " 'pars': 'pars',\n",
       " 'çıta': 'çı',\n",
       " 'mirket': 'mirket',\n",
       " 'şebek': 'şebek',\n",
       " 'hipopotam': 'hipopota',\n",
       " 'arslan': 'arsla',\n",
       " 'gepard': 'gepardı',\n",
       " 'emu': 'emu',\n",
       " 'kanguru': 'kangur',\n",
       " 'kivi': 'kiv',\n",
       " 'keselisıçan': 'keselisıça',\n",
       " 'ornitorenk': 'ornitorenk',\n",
       " 'tazmanyacanavarı': 'tazmanyacanavar',\n",
       " 'valabi': 'valabi',\n",
       " 'vombat': 'vombat',\n",
       " 'dodo': 'dodo',\n",
       " 'koala': 'koa',\n",
       " 'panda': 'pa',\n",
       " 'dalıcımartı': 'dalıcımar',\n",
       " 'karibu': 'karibu',\n",
       " 'misköküzü': 'misköküz',\n",
       " 'penguen': 'penguen',\n",
       " 'kutupayısı': 'kutupayıs',\n",
       " 'rengeyiği': 'rengeyik',\n",
       " 'fok': 'fok',\n",
       " 'mamut': 'mamut',\n",
       " 'tavuk': 'tavuk',\n",
       " 'inek': 'inek',\n",
       " 'eşek': 'eşek',\n",
       " 'gelincik': 'gelincik',\n",
       " 'keçi': 'keç',\n",
       " 'at': 'at',\n",
       " 'katır': 'ka',\n",
       " 'domuz': 'do',\n",
       " 'koyun': 'koy',\n",
       " 'hindi': 'h',\n",
       " 'boğa': 'bok',\n",
       " 'buzağı': 'buzak',\n",
       " 'büyükbaş': 'büyükbaş',\n",
       " 'bıldırcın': 'bıldırç',\n",
       " 'camız': 'ca',\n",
       " 'civciv': 'civciv',\n",
       " 'dana': 'da',\n",
       " 'horoz': 'horoz',\n",
       " 'kaz': 'kaz',\n",
       " 'ördek': 'ördek',\n",
       " 'sıpa': 'sıp',\n",
       " 'sığır': 'sığır',\n",
       " 'manda': 'ma',\n",
       " 'tosun': 'to',\n",
       " 'tay': 'tay',\n",
       " 'oğlak': 'oğlak',\n",
       " 'koç': 'koç',\n",
       " 'kuzu': 'kuz',\n",
       " 'küçükbaş': 'küçükbaş',\n",
       " 'hayvan': 'hayva',\n",
       " 'porsuk': 'porsuk',\n",
       " 'ayı': 'a',\n",
       " 'kunduz': 'kunduz',\n",
       " 'vaşak': 'vaşak',\n",
       " 'gelengi': 'gelengi',\n",
       " 'cougar': 'cougar',\n",
       " 'geyik': 'geyik',\n",
       " 'elk': 'elk',\n",
       " 'tilki': 'tilki',\n",
       " 'moose': 'moo',\n",
       " 'dağaslanı': 'dağasla',\n",
       " 'puma': 'p',\n",
       " 'tavşan': 'tavşa',\n",
       " 'rakun': 'rak',\n",
       " 'kokarca': 'kokar',\n",
       " 'sincap': 'sincap',\n",
       " 'kurt': 'kurt',\n",
       " 'bozayı': 'boza',\n",
       " 'dağkedisi': 'dağkedis',\n",
       " 'possum': 'poss',\n",
       " 'kayoti': 'çakal',\n",
       " 'aligator': 'aligator',\n",
       " 'timsah': 'timsah',\n",
       " 'yunus': 'yunus',\n",
       " 'balık': 'balık',\n",
       " 'kurbağa': 'kurbak',\n",
       " 'ıstakoz': 'ıstakoz',\n",
       " 'miskfaresi': 'miskfares',\n",
       " 'semender': 'semender',\n",
       " 'ahtapot': 'ahtapot',\n",
       " 'susamuru': 'susamur',\n",
       " 'istiridye': 'istiridye',\n",
       " 'salamandra': 'salamandra',\n",
       " 'denizaslanı': 'denizasla',\n",
       " 'köpekbalığı': 'köpekbalık',\n",
       " 'karakurbağası': 'karakurbağas',\n",
       " 'balina': 'bal',\n",
       " 'alg': 'alg',\n",
       " 'büyükbeyazköpekbalığı': 'büyükbeyazköpekbalık',\n",
       " 'akya': 'akya',\n",
       " 'carettacaretta': 'carettacaret',\n",
       " 'denizanası': 'denizanas',\n",
       " 'denizatı': 'denizat',\n",
       " 'denizkaplumbağası': 'denizkaplumbağas',\n",
       " 'denizyıldızı': 'denizyıldız',\n",
       " 'denizkestanesi': 'denizkestanes',\n",
       " 'derekurbağası': 'kurbak',\n",
       " 'kalamar': 'kalamar',\n",
       " 'istakoz': 'istakoz',\n",
       " 'karides': 'karides',\n",
       " 'mürekkepbalığı': 'mürekkepbalık',\n",
       " 'fokbalığı': 'fokbalık',\n",
       " 'sölenter': 'sölenter',\n",
       " 'suyosunu': 'suyo',\n",
       " 'yosun': 'yo',\n",
       " 'yengeç': 'yengeç',\n",
       " 'sukaplumbağası': 'sukaplumbağas',\n",
       " 'lama': 'la',\n",
       " 'öküz': 'ök',\n",
       " 'çinçilla': 'çinçilla',\n",
       " 'vizon': 'vizo',\n",
       " 'muhabbetkuşu': 'muhabbetkuş',\n",
       " 'kanarya': 'kanarya',\n",
       " 'kedi': 'kedi',\n",
       " 'köpek': 'köpek',\n",
       " 'gerbil': 'gerbil',\n",
       " 'goldenretriever': 'goldenretriever',\n",
       " 'ginedomuzu': 'ginedo',\n",
       " 'hamster': 'hamster',\n",
       " 'papağan': 'papağa',\n",
       " 'kondor': 'kondor',\n",
       " 'kartal': 'kartal',\n",
       " 'ispinoz': 'ispinoz',\n",
       " 'arapapağanı': 'arapapağa',\n",
       " 'pelikan': 'pelika',\n",
       " 'kızılgerdan': 'kızılger',\n",
       " 'tukan': 'tuka',\n",
       " 'ağaçkakan': 'ağaçkaka',\n",
       " 'akbaba': 'akbap',\n",
       " 'atmaca': 'atmaç',\n",
       " 'baykuş': 'baykuş',\n",
       " 'bülbül': 'bülbül',\n",
       " 'doğan': 'doğa',\n",
       " 'flamingo': 'flamingo',\n",
       " 'güvercin': 'güverç',\n",
       " 'karabatak': 'karabatak',\n",
       " 'keklik': 'keklik',\n",
       " 'karga': 'karga',\n",
       " 'leylek': 'leylek',\n",
       " 'kumru': 'kumru',\n",
       " 'kuğu': 'kuk',\n",
       " 'kuş': 'kuş',\n",
       " 'sülün': 'sül',\n",
       " 'turna': 'turna',\n",
       " 'şahin': 'şah',\n",
       " 'tavuskuşu': 'tavuskuş',\n",
       " 'serçe': 'serçe',\n",
       " 'saksağan': 'saksağa',\n",
       " 'martı': 'mar',\n",
       " 'kırlangıç': 'kırlangıç',\n",
       " 'yarasa': 'yaras',\n",
       " 'sümsükkuşu': 'sümsükkuş',\n",
       " 'yabanöküzü': 'yabanöküz',\n",
       " 'tibetsığırı': 'tibetsığır',\n",
       " 'kırkurdu': 'kırkur',\n",
       " 'karaca': 'karaç',\n",
       " 'gazel': 'gazel',\n",
       " 'jaguar': 'jaguar',\n",
       " 'oselo': 'oselo',\n",
       " 'vankedisi': 'vankedis',\n",
       " 'levrek': 'levrek',\n",
       " 'lepistes': 'lepistes',\n",
       " 'somon': 'somo',\n",
       " 'alabalık': 'alabalık',\n",
       " 'istavrit': 'istavrit',\n",
       " 'hamsi': 'hamsi',\n",
       " 'kefal': 'kefal',\n",
       " 'lüfer': 'lüfer',\n",
       " 'mezgit': 'mezgit',\n",
       " 'palamut': 'palamut',\n",
       " 'sazan': 'saza',\n",
       " 'pirana': 'pira',\n",
       " 'sardalya': 'sardalya',\n",
       " 'vantuzbalığı': 'balık',\n",
       " 'uskumru': 'uskumru',\n",
       " 'yılanbalığı': 'yılanbalık',\n",
       " 'çinekop': 'çinekop',\n",
       " 'çipura': 'çipur',\n",
       " 'karınca': 'kar',\n",
       " 'böcek': 'böcek',\n",
       " 'hamamböceği': 'hamamböcek',\n",
       " 'pire': 'pir',\n",
       " 'peygamberdevesi': 'peygamberdeves',\n",
       " 'akrep': 'akrep',\n",
       " 'arı': 'ar',\n",
       " 'ağustosböceği': 'ağustosböcek',\n",
       " 'eşekarısı': 'eşekarıs',\n",
       " 'güve': 'güv',\n",
       " 'kelebek': 'kelebek',\n",
       " 'kırkayak': 'kırkayak',\n",
       " 'sinek': 'sinek',\n",
       " 'sivrisinek': 'sivrisinek',\n",
       " 'solucan': 'soluca',\n",
       " 'çekirge': 'çekirge',\n",
       " 'tırtıl': 'tırtıl',\n",
       " 'uğurböceği': 'uğurböcek',\n",
       " 'sümüklüböcek': 'sümüklüböcek',\n",
       " 'tarantula': 'tarantul',\n",
       " 'yabanarısı': 'yabanarıs',\n",
       " 'örümcek': 'örümcek',\n",
       " 'çiyan': 'çiya',\n",
       " 'ipekböceği': 'ipekböcek',\n",
       " 'salyangoz': 'salyangoz',\n",
       " 'karafatma': 'karafatma',\n",
       " 'ökenek': 'sinek',\n",
       " 'karıncayiyen': 'karıncayiye',\n",
       " 'kirpi': 'kirpi',\n",
       " 'köstebek': 'köstebek',\n",
       " 'sivrifare': 'sivrifar',\n",
       " 'ape': 'ape',\n",
       " 'babun': 'bap',\n",
       " 'gibon': 'gibo',\n",
       " 'insan': 'in',\n",
       " 'marmoset': 'marmoset',\n",
       " 'orangutan': 'orangu',\n",
       " 'tembelhayvan': 'tembelhayva',\n",
       " 'coney': 'coney',\n",
       " 'yabanitavşan': 'yabanitavşa',\n",
       " 'pika': 'pika',\n",
       " 'geko': 'geko',\n",
       " 'iguana': 'iguan',\n",
       " 'kertenkele': 'kertenkel',\n",
       " 'yılan': 'yıla',\n",
       " 'tosbağa': 'tosbak',\n",
       " 'kaplumbağa': 'kaplumbak',\n",
       " 'anakonda': 'anako',\n",
       " 'dinozor': 'dinozor',\n",
       " 'ejderha': 'ejderha',\n",
       " 'kobrayılanı': 'kobrayıla',\n",
       " 'köryılan': 'kertenkel',\n",
       " 'piton': 'pito',\n",
       " 'sürüngen': 'sürünge',\n",
       " 'karakaplumbağası': 'karakaplumbağas',\n",
       " 'gopher': 'gopher',\n",
       " 'dağsıçanı': 'dağsıça',\n",
       " 'marmot': 'marmot',\n",
       " 'fare': 'fare',\n",
       " 'oklukirpi': 'oklukirpi',\n",
       " 'sıçan': 'sıça',\n",
       " 'lağımfaresi': 'fare',\n",
       " 'sansar': 'sansar',\n",
       " 'polecat': 'polecat'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('animal_list_snowball.pickle', 'rb') as handle:\n",
    "    anim_dict = pickle.load(handle)\n",
    "anim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25 model_1000_10_cbow  Threshold for Word2vec is: 0.16312341\n",
      "0.5 model_1000_10_cbow  Threshold for Word2vec is: 0.29870936\n",
      "0.75 model_1000_10_cbow  Threshold for Word2vec is: 0.4428481\n",
      "0.25 model_1000_10_skipgram  Threshold for Word2vec is: 0.13611728\n",
      "0.5 model_1000_10_skipgram  Threshold for Word2vec is: 0.1990204\n",
      "0.75 model_1000_10_skipgram  Threshold for Word2vec is: 0.2678917\n",
      "0.25 model_1000_4_cbow  Threshold for Word2vec is: 0.17810473\n",
      "0.5 model_1000_4_cbow  Threshold for Word2vec is: 0.32770053\n",
      "0.75 model_1000_4_cbow  Threshold for Word2vec is: 0.4660498\n",
      "0.25 model_1000_4_skipgram  Threshold for Word2vec is: 0.15042028\n",
      "0.5 model_1000_4_skipgram  Threshold for Word2vec is: 0.22613452\n",
      "0.75 model_1000_4_skipgram  Threshold for Word2vec is: 0.3018213\n",
      "0.25 model_300_10_cbow  Threshold for Word2vec is: 0.19835623\n",
      "0.5 model_300_10_cbow  Threshold for Word2vec is: 0.35215443\n",
      "0.75 model_300_10_cbow  Threshold for Word2vec is: 0.48747554\n",
      "0.25 model_300_10_skipgram  Threshold for Word2vec is: 0.23882182\n",
      "0.5 model_300_10_skipgram  Threshold for Word2vec is: 0.3368366\n",
      "0.75 model_300_10_skipgram  Threshold for Word2vec is: 0.4396623\n",
      "0.25 model_300_4_cbow  Threshold for Word2vec is: 0.21403752\n",
      "0.5 model_300_4_cbow  Threshold for Word2vec is: 0.370769\n",
      "0.75 model_300_4_cbow  Threshold for Word2vec is: 0.513079\n",
      "0.25 model_300_4_skipgram  Threshold for Word2vec is: 0.2574348\n",
      "0.5 model_300_4_skipgram  Threshold for Word2vec is: 0.3677264\n",
      "0.75 model_300_4_skipgram  Threshold for Word2vec is: 0.46594846\n",
      "0.25 model_600_10_cbow  Threshold for Word2vec is: 0.16793162\n",
      "0.5 model_600_10_cbow  Threshold for Word2vec is: 0.30488428\n",
      "0.75 model_600_10_cbow  Threshold for Word2vec is: 0.446571\n",
      "0.25 model_600_10_skipgram  Threshold for Word2vec is: 0.17441203\n",
      "0.5 model_600_10_skipgram  Threshold for Word2vec is: 0.25264293\n",
      "0.75 model_600_10_skipgram  Threshold for Word2vec is: 0.3338653\n",
      "0.25 model_600_4_cbow  Threshold for Word2vec is: 0.18158339\n",
      "0.5 model_600_4_cbow  Threshold for Word2vec is: 0.33221817\n",
      "0.75 model_600_4_cbow  Threshold for Word2vec is: 0.4651786\n",
      "0.25 model_600_4_skipgram  Threshold for Word2vec is: 0.18911257\n",
      "0.5 model_600_4_skipgram  Threshold for Word2vec is: 0.27452928\n",
      "0.75 model_600_4_skipgram  Threshold for Word2vec is: 0.36060825\n"
     ]
    }
   ],
   "source": [
    "#This code creates three clusters file for 0.25,0.50,0.75 percentages as a w2v_list_0.25.txt, w2v_list_0.5.txt, w2v_list_0.75.txt\n",
    "#Also creates sorted word2vec cosine similarity results for all animal pairs as a w2v_scores.csv\n",
    "#And creates scores file in a txt to check manually as a scr_list.txt. This file includes similarity scores for every person.\n",
    "\n",
    "#reading animal lists(cvf sequences)\n",
    "animals = pd.read_csv(\"animal.csv\") #reading a file\n",
    "data=animals[\"PREPROCESSED TEXT\"].values\n",
    "ids=animals[\"ID\"].values\n",
    "#loading models that found in previous step(we have 4 models in here)\n",
    "for m in models_files:\n",
    "    model = KeyedVectors.load_word2vec_format(m)\n",
    "    modelname=\"./word2vec_cluster_outputs/\"+str(m)[9:]\n",
    "    \n",
    "    #calculating similarity scores and assigning w2v_scores list\n",
    "    w2v_scores=[]\n",
    "    for i in data:\n",
    "        j=i.replace(\" \",\",\")\n",
    "        patient=list(j.split(\",\"))\n",
    "        while True:\n",
    "            try:\n",
    "                patient.remove('')\n",
    "            except:\n",
    "                break\n",
    "        for j in range(0,len(patient)-1):    \n",
    "            #word2vec cosine similarity\n",
    "            w1=patient[j]\n",
    "            w2=patient[j+1]\n",
    "            try:\n",
    "                w2v=model.similarity(anim_dict[w1],anim_dict[w2])\n",
    "                if w1==w2:\n",
    "                    continue\n",
    "                else:\n",
    "                    splitting=(str(w2v)+\"@\"+str(w1)+\"@\" +str(w2))\n",
    "                    if splitting not in w2v_scores:\n",
    "                        w2v_scores.append(str(w2v)+\"@\"+str(w1)+\"@\" +str(w2))\n",
    "            except:\n",
    "                print(w1,w2)         \n",
    "\n",
    "\n",
    "    #sorting similarity scores \n",
    "    w2v_scores.sort()\n",
    "\n",
    "    #writing sorted similarity scores to CSV file\n",
    "    score_file_name=modelname+\"_w2v_scores.csv\"\n",
    "    with open(score_file_name, \"w\", newline=\"\",encoding=\"utf-8\") as f:\n",
    "        wrt = csv.writer(f)\n",
    "        wrt.writerow([\"similarity_scores\",\"animal_1\", \"animal_2\"])\n",
    "        for i in range(0,len(w2v_scores)):\n",
    "            splitting=w2v_scores[i].split(\"@\")\n",
    "            wrt.writerow([splitting[0],splitting[1],splitting[2]])\n",
    "\n",
    "    #calculating threshold values for 0.25,0.50,0.75 percentage\n",
    "    threshold_list=[0.25,0.50,0.75]  \n",
    "    score_flag=True\n",
    "    count=0\n",
    "    for t in threshold_list:\n",
    "        w2v_switch=[]\n",
    "        ########## writing w2v_threshold values in a screen\n",
    "        w2v_threshold=w2v_scores[int(len(w2v_scores)*t)].split(\"@\")\n",
    "        print(t,modelname,\" Threshold for Word2vec is:\",w2v_threshold[0])\n",
    "        w2v_threshold=float (w2v_threshold[0])\n",
    "\n",
    "        ########creation of word2nvec clusters\n",
    "        big_animal_list2=[]\n",
    "        big_animal_list2_score=[]\n",
    "        for i in data:\n",
    "            j=i.replace(\" \",\",\")\n",
    "            patient=list(j.split(\",\"))\n",
    "            while True:\n",
    "                try:\n",
    "                    patient.remove('')\n",
    "                except:\n",
    "                    break\n",
    "            temp=[]\n",
    "            temp_score=[]\n",
    "            flag=0\n",
    "            animal_list=[]\n",
    "            for j in range(0,len(patient)-1):    \n",
    "                #calculating word2vec cosine similarity for creating clusters\n",
    "                w1=patient[j]\n",
    "                w2=patient[j+1]\n",
    "                try:\n",
    "                    w2v=model.similarity(anim_dict[w1],anim_dict[w2])\n",
    "                    temp_score.append(round(w2v,3))\n",
    "\n",
    "                    if w1==w2:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if w2v>=w2v_threshold:\n",
    "                            if flag==1:\n",
    "                                temp+=[w2]                \n",
    "                            else:\n",
    "                                temp+=[w1,w2]\n",
    "                                flag=1\n",
    "                        else:\n",
    "                            if temp!=[]:\n",
    "                                animal_list+=[temp]\n",
    "                                temp=[]\n",
    "                                flag=0\n",
    "                            else:\n",
    "                                animal_list+=[w1]\n",
    "                except:\n",
    "                    print(w1,w2)\n",
    "        #creating word2vec clusters and assign a list (temp list)\n",
    "            if temp!=[]:\n",
    "                animal_list+=[temp]\n",
    "            else:\n",
    "                animal_list+=[w2]\n",
    "\n",
    "            big_animal_list2.append(animal_list)\n",
    "            big_animal_list2_score.append(temp_score)\n",
    "        #################### make alone string names(singleton) as a group\n",
    "        w2v_list=[]\n",
    "        for i in big_animal_list2:\n",
    "            p=[]\n",
    "            for ii in i:\n",
    "                if ii.__class__ == str:\n",
    "                    p.append([ii])\n",
    "                else:\n",
    "                    p.append(ii)\n",
    "            w2v_list.append( p ) \n",
    "        #calculating switch numbers.(the number of created clusters-1 equals switch counts.)\n",
    "        for i in w2v_list:\n",
    "            w2v_switch.append(len(i)-1)\n",
    "        #assigning switch numbers to pnada dataframe\n",
    "        if score_flag:\n",
    "            df_switch = pd.DataFrame(w2v_switch, columns = [t]) \n",
    "        else:\n",
    "            df_switch.insert(count,str(t), w2v_switch)\n",
    "\n",
    "\n",
    "        w2v_list_name=modelname+\"_w2v_list_\"+str(t)+\".txt\"\n",
    "        #writing w2v_list to CSV file. Can be seen created all clusters between bracelets[]\n",
    "        ths = open(w2v_list_name, \"w\", encoding=\"utf-8\")\n",
    "        #ths.write(\"ID@CLUSTER\\n\")\n",
    "        for sayac,i in enumerate(w2v_list):\n",
    "            #temp=str(ids[sayac])+\"@\"+str(i)+\"\\n\"\n",
    "            temp=str(i)+\"\\n\"\n",
    "            ths.write(temp)\n",
    "        ths.close()\n",
    "        count=count+1\n",
    "        #writing scores in a txt file to check clusters manually-optional\n",
    "        if score_flag:\n",
    "            scr_list_name=modelname+\"_scr_list.txt\"\n",
    "            ths = open(scr_list_name, \"w\", encoding=\"utf-8\")\n",
    "            #ths.write(\"ID@CLUSTER\\n\")\n",
    "            for sayac,i in  enumerate(big_animal_list2_score):\n",
    "                #temp=str(ids[sayac])+\"@\"+str(i)+\"\\n\"\n",
    "                temp=str(i)+\"\\n\"\n",
    "                ths.write(temp)\n",
    "            ths.close()\n",
    "            score_flag=False\n",
    "    switchcounts_name=modelname+\"_w2v_switch.csv\"\n",
    "    #df_switch[\"ID\"]=ids\n",
    "    #writing switch numbers(threshold 0.25,0.50 and 0.75 by percentage) in a csv file.the file name is modelname_w2v_switch.csv\n",
    "    df_switch.to_csv(switchcounts_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
